# rag-fastapi

A Retrieval-Augmented Generation (RAG) pipeline with FastAPI and Mistral AI, built from scratch without external RAG or search libraries.

## ğŸš€ Project Status

**Phase 1: Complete âœ…** - Project setup and infrastructure

### What's Working:
- âœ… Complete project structure
- âœ… Configuration management with Pydantic
- âœ… FastAPI application skeleton
- âœ… API endpoint stubs
- âœ… Development environment ready

### Coming Next:
- ğŸ”¨ Phase 2: Data ingestion pipeline
- ğŸ”¨ Phase 3: Search implementation
- ğŸ”¨ Phase 4: Query processing & LLM
- ğŸ”¨ Phase 5: Bonus features
- ğŸ”¨ Phase 6: UI development

## ğŸ“‹ Features (Planned)

- **PDF Ingestion**: Upload and process PDF documents
- **Smart Chunking**: Intelligent text splitting with overlap
- **Hybrid Search**: Semantic (embeddings) + Keyword (BM25)
- **Custom Vector Store**: No external vector database needed
- **Intent Detection**: Smart query routing
- **Answer Generation**: Powered by Mistral AI
- **Source Citations**: Transparent answer sourcing
- **Vanilla JS UI**: Clean, simple chat interface

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.9+
- pip

### Setup

1. **Clone the repository**
```bash
git clone <repository-url>
cd rag-fastapi
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment**
```bash
cp .env.example .env
# Edit .env with your settings (Mistral API key is pre-configured)
```

## ğŸš€ Running the Application

### Development Server

```bash
python run.py
```

Or using uvicorn directly:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Access Points
- **Web Interface**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health

## ğŸ“ Project Structure

```
rag-fastapi/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI application entry point
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ingestion.py     # Document upload endpoints
â”‚   â”‚   â””â”€â”€ query.py         # Query endpoints
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration management
â”‚   â”‚   â”œâ”€â”€ chunking.py      # Text chunking (Coming in Phase 2)
â”‚   â”‚   â”œâ”€â”€ embeddings.py    # Embedding generation (Phase 2)
â”‚   â”‚   â”œâ”€â”€ search.py        # Hybrid search (Phase 3)
â”‚   â”‚   â”œâ”€â”€ ranking.py       # Result re-ranking (Phase 3)
â”‚   â”‚   â””â”€â”€ llm.py           # Mistral AI client (Phase 4)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py       # Pydantic models
â”‚   â””â”€â”€ storage/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ vector_store.py  # Custom vector storage (Phase 2)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html           # Web UI (Phase 6)
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ style.css
â”‚       â””â”€â”€ app.js
â”œâ”€â”€ data/                    # Vector store storage
â”œâ”€â”€ uploads/                 # Temporary PDF uploads
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env.example            # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ plan.md                 # Detailed implementation plan
â”œâ”€â”€ task.md                 # Original task description
â”œâ”€â”€ run.py                  # Convenience run script
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

Edit `.env` file to configure:

```env
# API Keys
MISTRAL_API_KEY=your_api_key_here

# Chunking
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# Search
TOP_K_RESULTS=5
SIMILARITY_THRESHOLD=0.6
SEMANTIC_WEIGHT=0.7
KEYWORD_WEIGHT=0.3

# Models
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
LLM_MODEL=mistral-medium
```

## ğŸ“š API Endpoints

### Status
- `GET /health` - Health check
- `GET /api/status` - System statistics

### Ingestion (Coming in Phase 2)
- `POST /api/ingest` - Upload PDF files

### Query (Coming in Phase 4)
- `POST /api/query` - Query the knowledge base

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=app tests/
```

## ğŸ“– Documentation

For detailed implementation plan, see [plan.md](plan.md).

## ğŸ¯ Design Decisions

### Why No External Libraries for RAG/Search?
Per project requirements, we implement core RAG components from scratch:
- **Custom Vector Store**: Numpy-based with pickle serialization
- **BM25 Implementation**: From-scratch keyword search
- **Hybrid Search**: Manual combination of semantic + keyword

### Tech Stack Choices
- **FastAPI**: Modern, fast, automatic API docs
- **Sentence Transformers**: Quality embeddings, runs locally
- **Mistral AI**: Powerful LLM with good API
- **Vanilla JS**: Simple, no build process needed

## ğŸ“ License

MIT

## ğŸ‘¤ Author

Built as a demonstration project for RAG pipeline implementation.

---

**Note**: This is Phase 1. Core functionality (ingestion, search, query) coming in subsequent phases.
